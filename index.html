<html>
<head>
  <meta content="text/html; charset=utf-8" http-equiv="Content-Type" />
  <title>Bin Sun</title>
  <meta content="Bin Sun, Sun1992.github.io" name="keywords" />
  <style media="screen" type="text/css">html, body, div, span, applet, object, iframe, h1, h2, h3, h4, h5, h6, p, blockquote, pre, a, abbr, acronym, address, big, cite, code, del, dfn, em, font, img, ins, kbd, q, s, samp, small, strike, strong, sub, tt, var, dl, dt, dd, ol, ul, li, fieldset, form, label, legend, table, caption, tbody, tfoot, thead, tr, th, td {
  border: 0pt none;
  font-family: inherit;
  font-size: 100%;
  font-style: inherit;
  font-weight: inherit;
  margin: 0pt;
  outline-color: invert;
  outline-style: none;
  outline-width: 0pt;
  padding: 0pt;
  vertical-align: baseline;
}
a {
  color: #1772d0;
  text-decoration:none;
}
a:focus, a:hover {
  color: #f09228;
  text-decoration:none;
}
a.paper {
  font-weight: bold;
  font-size: 14pt;
}
b.paper {
  font-weight: bold;
  font-size: 14pt;
}
* {
  margin: 0pt;
  padding: 0pt;
}
body {
  position: relative;
  margin: 3em auto 2em auto;
  width: 1000px;
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 16px;
  background: #eee;
}
h2 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 17pt;
  font-weight: 700;
}
h3 {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 18px;
  font-weight: 700;
}
strong {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight:bold;
}
ul { 
  list-style: circle;
}
img {
  border: none;
}
li {
  padding-bottom: 0.5em;
  margin-left: 1.4em;
}
alert {
  font-family: Lato, Verdana, Helvetica, sans-serif;
  font-size: 15px;
  font-weight: bold;
  color: #FF0000;
}
em, i {
  font-style:italic;
}
div.section {
  clear: both;
  margin-bottom: 1.5em;
  background: #eee;
}
div.spanner {
  clear: both;
}
div.paper {
  clear: both;
  margin-top: 0.5em;
  margin-bottom: 1em;
  border: 1px solid #ddd;
  background: #fff;
  padding: 1em 1em 1em 1em;
}
div.paper div {
  padding-left: 230px;
}
img.paper {
  margin-bottom: 0.5em;
  float: left;
  width: 200px;
}
span.blurb {
  font-style:italic;
  display:block;
  margin-top:0.75em;
  margin-bottom:0.5em;
}
pre, code {
  font-family: 'Lucida Console', 'Andale Mono', 'Courier', monospaced;
  margin: 1em 0;
  padding: 0;
}
div.paper pre {
  font-size: 0.9em;
}
</style>

<link href="http://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic" rel="stylesheet" type="text/css" /><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans+Condensed:300' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Open+Sans' rel='stylesheet' type='text/css'>--><!--<link href='http://fonts.googleapis.com/css?family=Yanone+Kaffeesatz' rel='stylesheet' type='text/css'>-->
</head>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-45959174-3', 'kailigo.github.io');
  ga('send', 'pageview');
</script>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-66888300-1', 'auto');
  ga('send', 'pageview');
</script>
<body>
<div style="margin-bottom: 1em; border: 1px solid #ddd; background-color: #fff; padding: 1em; height: 140px;">
<div style="margin: 0px auto; width: 100%;">
<img title="Bin" style="float: left; padding-left: .5em; height: 140px;" src="Bin.png" />
<div style="padding-left: 12em; vertical-align: top; height: 120px;"><span style="line-height: 150%; font-size: 20pt;">Bin Sun</span><br />
<span><strong>Ph.D. from Northeastern University</strong></span><br />
<!--<span><a href='http://cvrs.whu.edu.cn/'>Computer Vision & Remote Sensing (CVRS) Lab </a> <br /> </q>-->
<span>SMILELab</span><br />
<!-- <span><strong>Office</strong>: Richard Center, 360 Huntington Ave, Boston, MA 02115</span><br /> -->
<span><strong>Email  </strong>: sun.bi [at] northeastern.edu <br />
<strong><a href='https://github.com/Sun1992'>Github</a></strong>  &nbsp &nbsp
<strong><a href='https://scholar.google.com/citations?user=7s-TBDUAAAAJ&hl=en'>Google Scholar</a></strong></span><br/>

<!-- <span><strong> <a href='https://github.com/kailigo'>Github</a> </strong></span> <br /> -->
<!-- <span><strong> <a href='https://scholar.google.com/citations?hl=en&user=YsROc4UAAAAJ'>Google Scholar</a> </strong></span> <br /> -->
</div>
</div>
</div>
<!--<div style="clear: both; background-color: #fff; margin-top: 1.5em; padding: .2em; padding-left: .3em;">-->

<div style="clear: both;">
<div class="section">
<h2>About Me</h2>
<div class="paper">
<!--I am a Research Scientist at Facebook Reality Labs (<a href='https://research.fb.com/people/li-kunpeng/'>FB page</a>). -->
I am a Senior Research Scientist at <a href='https://picsart.com'><strong>TopazLabs</strong></a>. My current research interests lie in Generative AI and efficient model.
 <br />  <br /> I received my Ph.D degree from Electrical and Computer Engineering Department, <a href='http://www.northeastern.edu/'>Northeastern University</a>, advised by <a href='http://www1.ece.neu.edu/~yunfu/'> Prof. Yun Fu</a>. During my Ph.D. career, I focus on studying Computer Vision and Deep Learning, especially learning with efficient model, Face and Pose Estimation, Multi-modality, super-resolution, agriculture vision, etc. Please find the webpage of our lab here <a href='https://web.northeastern.edu/smilelab/'>SmileLab</a>.    
I have also spent time as a founding member in AI-startups (<a href='https://www.businesswire.com/news/home/20171107006109/en/Shiseido-Americas-Acquires-Giaran-Inc.'>Giaran acquired by Shiseido</a>, <a href='https://ainnovationlabs.com'>Ainnovationlabs</a>).
Besides, I spent three months as a computer vision engineer in
<a href='https://www.zebra.com/us/en.html'>Zebra</a>.
 

<!-- Prior to this, I obtained my bachelor degree from School of Electronic and Information Engineering, <a href='https://www.scut.edu.cn/new/'>South China University of Technology</a>, supervised by Prof. Xin Zhang and <a href='http://www.hcii-lab.net/lianwen/'>Prof. Lianwen Jin</a>. -->


</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
  <h2>News</h2>
  <div class="paper">
    <ul>
<!--   <li> 2018.07: One paper is accepted by <strong> ECCV 2018 </strong>.</li> 
  <li> 2018.07: We have one paper accepted by <strong> ACM MM 2018 </strong> .</li>  -->
  <li> 2024.2: Start my new position of Senior Research Scientist at Picsart!</li>
  <li> 2023.11: Release the first <a href='https://community.topazlabs.com/t/generative-remove-with-local-processing-november-2023/55465/1'><strong>image editing product</strong></a> of our company! We deploy the <strong>Stable Diffusion</strong> on local laptops.</li>
  <li> 2023.06: Goodbye Boston. Hello Dallas.</li>
  <li> 2023.05:Get an offer from <a href='https://www.topazlabs.com'><strong>TopazLabs</strong></a> as a <strong>Deep Learning Researcher</strong>. The topic is about <strong>Image Generation</strong>.</li>
  <li> 2023.01:One paper is accepted by <strong>ICLR</strong> as Oral.</li>
  <li> 2022.12: Defense my PhD and graduate from <strong>Northeastern University</strong></li>
  <li> 2022.11:One paper is accepted by <strong>AAAI</strong>.</li>
  <li> 2022.03:One paper is accepted by <strong>CVPR</strong> as oral.</li>
  <li> 2021.11: One paper is accepted by <strong>IEEE TNNLS</strong>.</li>
  <li> 2021.10: Start part-time intern in <strong>Ainnovationlabs</strong>.</li>
  <li> 2021.06: Lead the team won the <strong>4th Rank</strong> in CVPR21 Challenge on Agriculture Vision.</li>
  <li> 2021.03: Lead the team won the <strong>1st Prize</strong> in both RGB and RGB-D tracks of CVPR21 Challenge on Large Scale Signer Independent Isolated Sign Language Recognition <a href='https://github.com/jackyjsy/CVPR21Chal-SLR'>[code]</a> <a href='http://chalearnlap.cvc.uab.es/challenge/43/description/'>[link]</a> <a href='https://coe.northeastern.edu/news/smile-lab-wins-1st-at-conference-on-computer-vision-and-pattern-recognition/'>[news]</a>.</li>
  <li> 2020.11: Two papers are accepted by <strong>FG</strong>.</li>
  <li> 2020.03: One patent is filed by <strong>Zebra</strong>.</li>
  <li> 2019.06: One patent is filed by <strong>Shiseido</strong>.</li>
  <li> 2019.05: Involved in another startup <strong>Ainnovaionlabs, Inc</strong>.</li>
  <li> 2019.01: Start my internship at <strong>Zebra</strong>, Lincolnshire, IL.</li>
  <li> 2018.07: Get one paper accepted by <strong> BMVC </strong>.</li>
  <li> 2018.04: Get one paper accepted by <strong> AAAI </strong>.</li>
  <li> 2017.11: Giaran is acquired by <strong> Shiseido </strong>.</li>
  <li> 2017.01: Start my internship at <strong>Giaran</strong> as one of the founding members.</li>
  <li> 2016.09: Change my program to <strong> Ph.D </strong>.</li>
  <li> 2015.09: Begin my new journey at <strong>Northeastern University</strong>, Boston, MA.</li>
    </ul>
  </div>
</div>
</div>




<!-- <div style="clear: both;">
<div class="section">
<h2 id="confpapers">Intern Experience</h2>


 -->


<div style="clear: both;">
<div class="section">
<!-- <h2 id="confpapers">Previous Selected Publications</h2> -->
  <h2>Selected Publications [<a href='https://scholar.google.com/citations?user=7s-TBDUAAAAJ&hl=en'>Full List and Recent Works</a>]</h2>

  
  <div class="paper" id="CoCs"><img class="paper" src="./pic/CoCs.png" title="Image as Set of Points">
  <div> <strong>Image as Set of Points</strong><br>
      Xu Ma*, Yuqian Zhou*, Huan Wang, Can Qin, <strong><u>Bin Sun</u></strong>, Chang Liu, Yun Fu <br>
      International Conference on Learning Representations (<strong>ICLR</strong>), 2023 <strong>top 5% Oral</strong>
  <a href='https://arxiv.org/pdf/2303.01494'>[PDF]</a>,
  <a href='https://github.com/ma-xu/Context-Cluster'>[Code]</a>
  </div>
  <div class="spanner"></div>
  </div>
  
<div class="paper" id="hpun"><img class="paper" src="./pic/HPUN.png" title="Hybrid pixel-unshuffled network for lightweight image super-resolution">
<div> <strong>Hybrid pixel-unshuffled network for lightweight image super-resolution</strong><br>
<strong><u>Bin Sun</u></strong>, Yulun Zhang, Songyao Jiang, Yun Fu <br>
Association for the Advancement of Artificial Intelligence (<strong>AAAI</strong>), 2023
<a href='https://arxiv.org/pdf/2203.08921.pdf'>[PDF]</a>,
<a href='https://github.com/Sun1992/HPUN'>[Code]</a>,
<a href='https://youtu.be/WzTY4diSeoA'>[Demo]</a>
</div>
<div class="spanner"></div>
</div>
  
  <div class="paper" id="LIVE"><img class="paper" src="./pic/LIVE.png" title="Towards Layer-wise Image Vectorization">
  <div> <strong>Towards Layer-wise Image Vectorization</strong><br>
      Xu Ma, Yuqian Zhou, Xingqian Xu, <strong><u>Bin Sun</u></strong>, Valerii Filev, Nikita Orlov, Yun Fu, Humphrey Shi <br>
      IEEE/CVF Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>), 2022 <strong>Oral</strong>
  <a href='https://openaccess.thecvf.com/content/CVPR2022/papers/Ma_Towards_Layer-Wise_Image_Vectorization_CVPR_2022_paper.pdf'>[PDF]</a>,
  <a href='https://github.com/Picsart-AI-Research/LIVE-Layerwise-Image-Vectorization'>[Code]</a>
  </div>
  <div class="spanner"></div>
  </div>
  
  
  
<div class="paper" id="LRPRNet"><img class="paper" src="./pic/LRPRNet.png" title="Visual Semantic Reasoning for Image-Text Matching">
<div> <strong>LRPRNet: Lightweight Deep Network by Low-Rank Pointwise Residual Convolution</strong><br>
<strong><u>Bin Sun</u></strong>, Jun Li, Ming Shao, Yun Fu <br>
IEEE Transactions on Neural Networks and Learning Systems (<strong>TNNLS</strong>), 2021
<a href='https://ieeexplore.ieee.org/iel7/5962385/6104215/09629225.pdf'>[PDF]</a>,
<a href='https://patentimages.storage.googleapis.com/b6/b1/03/e9d2e9c1bc0114/US20220156554A1.pdf'>[Patent]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="SAMSLR"><img class="paper" src="./pic/SAMSLR.png" title="Skeleton aware multi-modal sign language recognition">
<div> <strong>Skeleton aware multi-modal sign language recognition</strong><br>
    Songyao Jiang*, <strong><u>Bin Sun*</u></strong>, Lichen Wang, Yue Bai, Kunpeng Li, Yun Fu <br>
    IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshop (<strong>CVPRW, 1st Prize</strong>), 2021
<a href='https://openaccess.thecvf.com/content/CVPR2021W/ChaLearn/papers/Jiang_Skeleton_Aware_Multi-Modal_Sign_Language_Recognition_CVPRW_2021_paper.pdf'>[PDF]</a>,
<a href='https://github.com/jackyjsy/CVPR21Chal-SLR'>[Code]</a>

<!-- [code]</a> -->
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="EVdataset"><img class="paper" src="./pic/EVDataset.png" title="Ev-action: Electromyography-vision multi-modal action dataset">
<div> <strong>Ev-action: Electromyography-vision multi-modal action dataset</strong><br>
Lichen Wang, <strong><u>Bin Sun</u></strong>, Joseph Robinson, Taotao Jing, Yun Fu <br>
IEEE International Conference on Automatic Face and Gesture Recognition (<strong>FG</strong>), 2020
<a href='https://ieeexplore.ieee.org/iel7/9320148/9320149/09320160.pdf'>[PDF]</a>
</div>
<div class="spanner"></div>
</div>



<div class="paper" id="3DDHM"><img class="paper" src="./pic/3DDHM.png" title="Deep Evolutionary 3D Diffusion Heat Maps for Large-pose Face Alignment">
<div> <strong>Deep Evolutionary 3D Diffusion Heat Maps for Large-pose Face Alignment</strong><br>
<strong><u>Bin Sun</u></strong>, Ming Shao, Siyu Xia, Yun Fu <br>
British Machine Vision Conference (<strong>BMVC</strong>), 2018 <!--<strong>(Spotlight)</strong>-->
<a href='http://www.bmva.org/bmvc/2018/contents/papers/0904.pdf'>[PDF]</a>,
<a href='https://patentimages.storage.googleapis.com/7e/1a/9f/520341ec34ad60/US11000107.pdf'>[Patent]</a>
</div>
<div class="spanner"></div>
</div>


<!-- <div class="paper" id="RCAN"><img class="paper" src="./pic/RCAN.png" title="Image Super-Resolution Using Very Deep Residual Channel Attention Networks">
<div> <strong>Image Super-Resolution Using Very Deep Residual Channel Attention Networks</strong><br>
Yulun Zhang, <strong><u>Kunpeng Li</u></strong>, Kai Li, Bineng Zhong, Yun Fu <br>
European Conference on Computer Vision (<strong>ECCV</strong>), 2018
<a href='https://arxiv.org/abs/1807.02758'>[PDF]</a>,
<a href='https://github.com/yulunzhang/RCAN'>[Code]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="SN_reID"><img class="paper" src="./pic/SN_reID.png" title="Support Neighbor Loss for Person Re-Identification">
<div> <strong>Support Neighbor Loss for Person Re-Identification</strong><br>
Kai Li, Zhengming Ding, <strong><u>Kunpeng Li</u></strong>, Yulun Zhang, Yun Fu <br>
ACM Multimedia (<strong>ACM MM</strong>), 2018
<a href='https://arxiv.org/abs/1808.06030'>[PDF]</a>,
<a href='https://github.com/kailigo/SN_loss_for_reID'>[Code]</a>
</div>
<div class="spanner"></div>
</div>
 

<div class="paper" id="MDSLT"><img class="paper" src="./pic/MDSLT.png" title="Multi-stream Deep Similarity Learning Networks for Visual Tracking">
<div> <strong>Multi-stream Deep Similarity Learning Networks for Visual Tracking</strong><br>
<strong><u>Kunpeng Li</u></strong>, Yu Kong, Yun Fu <br>
International Joint Conference on Artificial Intelligence (<strong>IJCAI</strong>), 2017
<a href='https://www.ijcai.org/proceedings/2017/0301.pdf'>[PDF]</a>,
<a href='https://drive.google.com/file/d/1WoUK3G4khzI_qw1T48hvxa-7yPYYv3Ua/view'>[Benchmark Results]</a>,
<a href='https://www.youtube.com/watch?v=UgrwdRQYAIA'>[Video Results]</a>
</div>
<div class="spanner"></div>
</div>


<div class="paper" id="AirWriting"><img class="paper" src="./pic/AirWriting.png" title="A New Fingertip Detection and Tracking Algorithm and Its Application on Writing-in-the-air System">
<div> <strong>A New Fingertip Detection and Tracking Algorithm and Its Application on Writing-in-the-air System</strong><br>
<strong><u>Kunpeng Li</u></strong>, Xin Zhang <br>
IEEE International Conference on Image and Signal Processing, 2014
<a href='https://ieeexplore.ieee.org/abstract/document/7003824'>[PDF]</a>,
<a href='https://drive.google.com/file/d/0B6GPXWGb8uiyUW8tN2tmUERxWmc/view'>[Extension]</a>,
<a href='https://www.youtube.com/watch?v=gCHIq0OQwUM'>[Demo]</a>
</div>
<div class="spanner"></div>
</div>

-->

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Patents</h2>
<div class="paper">
<ul>
<p><font size="5">
    <li>  <strong>US20190014884A1</strong> systems and Methods for Virtual Facial Makeup Removal and Simulation, Fast Facial Detection and Landmark Tracking, Reduction in Input Video Lag and Shaking, and a Method for Recommending Makeup (<strong>Granted by Shiseido</strong>)  </li>
    <li>  <strong>WO2019213459A1</strong> System and Method for Generating Image Landmarks </li>
    <li> <strong>US11210549B2</strong> Automated chute fullness detection (<strong>Granted by Zebra</strong>) </li>
    <li> <strong>WO2020247545A1</strong> Light-weight decompositional convolution neural network </li>
    <li> <strong>WO2021163103A1</strong> Light-weight pose estimation network with multi-scale heatmap fusion </li>
     <li> <strong>US20230153946A1</strong>System and Method for Image Super-Resolution</li>
</font></p>
</ul>
</div>
</div>
</div>



<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Awards</h2>
<div class="paper">
    <ul>
  
  <li>  <strong>4th place</strong> in CVPR21 Challenge on Agriculture Vision 2021. </li>
  <li> <strong>1st Prize</strong> in both RGB and RGB-D tracks of CVPR21 Challenge on Large Scale Signer Independent Isolated Sign Language Recognition <a href='https://github.com/jackyjsy/CVPR21Chal-SLR'>[code]</a> <a href='http://chalearnlap.cvc.uab.es/challenge/43/description/'>[link]</a> <a href='https://coe.northeastern.edu/news/smile-lab-wins-1st-at-conference-on-computer-vision-and-pattern-recognition/'>[news]</a>, 2021 </li>
  <li> <strong>Star Intern of the Year</strong>, Zebra Corp, USA, 2019. </li>
  <li> <strong>GapFund360 Award</strong>, Northeastern University, USA, 2018. </li>
<!--  <li> NSF travel award for attending IJCAI, 2017. </li>
  <li> Dean's Fellowship, College of Engineering, Northeastern University, 2016. </li>
  <li> First Prize in Digital Image Processing Contest of Guangdong Province, 2015. </li>
  <li> National Scholarship, South China University of Technology, 2014. </li>
    <li> Meritorious Winner in International Mathematical Contest in Modeling, 2014. </li>  
    <!-- <li> Best Undergraduate Thesis, School of Remote Sensing and Information Engineering, Wuhan University, 2014. </li> -->
    <!-- <li> Excellent Undergraduate Students, Wuhan University, 2012. </li> -->
    </ul>
</ul>
</div>
</div>
</div>

<div style="clear: both;">
<div class="section">
<h2 id="confpapers">Professonal Activities</h2>
<div class="paper">
<ul>
<p><font size="5">
  <li> <strong>PC Member or Reviewer</strong> for CVPR, ICCV, AAAI, IJCAI, FG, TPAMI, TIP, TCSVT, IET Image Processing, JEI, CIM </li>
  <!-- <li> External Reviewer for CVPR, ICCV, ECCV, AAAI, IJCAI, AAAI (2017). </li> -->
    <!-- <li> Guest reviewer for AAAI 2017. </li>       -->
  <!-- <li> IEEE student member, AAAI student member. </li>    -->
</font></p>
</ul>
</div>
</div>
</div>



</body>
</html>
